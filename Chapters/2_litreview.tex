%!TEX root = ../thesis_rui_almeida.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% 2_litreview.tex
%% Rui V. Almeida's thesis file
%%
%% This chapter contains the literature review.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Literature Review}%
\label{cha:literature_review}

\section{\acrlong{AP}}%
\label{sec:ap}

Daniel Vallero, in his book "Fundamentals of Air
Pollution"~\cite{Vallero2014} makes a very important observation: Air
Pollution has no universal definition. Its meaning is intertwined with
the context with which it is measured and observed, with the ecosystem
in which it is perceived and even with the pollutant concentration (not
every toxic compound is toxic at every concentration). The \gls{EPA}
defines Air Pollution as the following:

\begin{center}
    \begin{minipage}{0.8\textwidth}

        \noindent \textit{Air Pollution is the presence of contaminants
        or pollutant substances in the air that interfere with human
        health or welfare, or produce other harmful environmental effects.}

    \end{minipage}
\end{center}

He then analyzes this definition through two possible lenses, the one
that comes with the interference produced by air contaminants; and the
one that comes from the harm they may cause. He notes that both points
of view come with a heavy burden of ambiguity, incompatible with a
scientific definition. We can thus observe that preferable to address
the issue through its measurable effects and consequences. These are
well-established and well known, and scientists all around the world
have been publishing extensively about them for some decades now. The
correlation between Air Pollution and an increased mortality in heavily
industrialized areas was first established in Europe, in the
19\textsuperscript{th} century, but the first time it was taken
seriously was during the 1952 killer-smog incidents, in
London~\cite{Platt2007}. At the time, a combination of very cold
weather, an anticyclone and fireplace emissions caused a thick smog to
fall over London, directly causing thousands of deaths and indirectly
many more~\cite{Bell2008,Office2019}. The disastrous consequences of
this incident had a huge impact in the civil society, resulting in a
series of policies and laws, among which the Clean Air Acts of 1956 and
1968, which are broadly considered to be some of the first actions to
decrease pollution in human societies. Much work has been done, and it
has resulted in remarkable progress since the definition of those two
policies. We are in fact in a much better place than we were some years
or decades ago, but pollution is still a part of everyday reality for
the whole of civilization. In the current day and age, both European and
American regulatory and surveillance bodies (the \gls{EEA} and the
\gls{EPA}, respectively) have identified a group of six \emph{criteria
pollutants} that need to be monitored effectively. These gases, whose
effects this section particularly focuses, are presented in
Table~\ref{tab:criteria_pollutants}. In this section, I will present the
most significant aspects of \gls{AP} that are described in the
literature, including health effects, environmental effects and
\acrlong{AP} monitoring.

\begin{table}[htpb]
    \centering
    \caption{Criteria pollutants as defined by the EPA and the
    EEA~\cite{CABI2019}. These are the pollutants whose effect is more
    significant for society itself, given their level of dangerousness and
    how common they are.}
    \label{tab:criteria_pollutants}
    \begin{tabular}{c}
    
    \end{tabular}
\end{table}

\todo[inline]{Criteria Pollutant table}


\subsection{\acrlong{AP} Effects on Human Health}%
\label{sub:ap_effects_on_human_health}

Arguably, there is no medium in which it is more important to consider
\gls{AP} by its effects than in the human body. However, even this has
its caveats. The body's response to any given substance changes with the
dose that is administered to it, something which has been known to us
for centuries:

\begin{flushright}
    \begin{minipage}{0.8\textwidth}
        \noindent
        \textit{
            What is it that is not poison? All things are poison and
            nothing is without poison. It is the dose alone that makes a
            thing not poison.
        }

        \hfill-- Paracelsus
    \end{minipage}
\end{flushright}

This quote, originally in the writings of one of the fathers of modern
medicine, the Swiss Paracelsus, was taken from Patricia Frank's book
called \emph{The Dose Makes The Poison}~\cite{Frank2011} and is one of
the core tenets of toxicology even today. There are, however, some
substances which do not need anything close to a high dose to cause harm
to human health, and in general, atmospheric pollutants fall in that
category. According to the \gls{EEA}, heart disease and stroke are the
most common causes of premature death due to \acrlong{AP}. The same
organization states that the most prominent atmospheric pollutants in
terms of the effects they have on human health are \gls{PM}, \gls{no2}
and \gls{o3}~\cite{EEA2016, EEA2007}. In this thesis, I will focus
mostly on them, not only because of their health importance, but also
because of their spectral nature, which allows us to detect them using
\gls{DOAS}~\cite{Platt2007}. Of course, a complete description of how
\gls{AP} affects the human body is a colossal task which is well beyond
the scope of this thesis.  Therefore, I will focus my attention on the
more prominent symptoms that are results of these chemicals: respiratory
syndromes, cardiovascular diseases, problems during gestation and
finally, neurologic consequences of \gls{AP}.

%%% THIS PARAGRAPH IS WELL WRITTEN, BUT DID NOT FIT THAT WELL
%%%%%%%%%%%%%%%%%%%%%%%%%%%

% It is not the dose alone to make the poison. Rather, one has to consider
% the whole context at which we are directing our discussion. Nitrogen
% compounds in the air cause several respiratory syndromes, but in the
% soil they are essential nutrients~\cite{Vallero2014, Lovett2009}.
% Admittedly this is an extreme comparison, but it serves to introduce the
% reader to the concept of dependence between the poisonous nature of a
% chemical and the exposure conditions and circumstances. Moreover, it is
% the precursor to another important notion, which is that of the relation
% between dose and response. Generally, one can express the adverse effect
% of a given chemical by the damage it causes in relation to the exposure,
% the dose. Hence, increasing the concentration of a chemical in an
% organism also increases the severity of the adverse
% outcome~\cite{Vallero2014}. This mechanism, in which a variation of dose
% implies a change in the organism's response is what is called the
% dose-response, and is an important chemical characterization method in
% this context.

\subsubsection{Respiratory effects of \acrlong{AP}}%
\label{ssub:respiratory_system_ailments_related_to_ap}

The respiratory system's main functions are the delivery of oxygen into
the blood stream and the removal of carbon dioxide from the body. Air
enters the body from the upper airways and flows to the alveolar region,
where oxygen diffuses across the lung wall into the blood stream, from
which it is transported to the tissues where it diffuses yet again and
is made available to the mitochondria in the cells, that use it for
cellular respiration~\cite{Nilsson2010}. The whole system is in
permanent interaction with the atmosphere, and is therefore exposed to
all kinds of air pollutants and trace gases, and therefore it is only
natural that respiratory effects are among the most direct health
complications originating in \gls{AP}~\cite{Vallero2014}.

The region in which a given pollutant is, within the respiratory system
(see Figure~\ref{fig:respiratory_system}), is of great importance. After
the air is inhaled through the nose, the air is heated or cooled to body
temperature, as well as humidified, in the upper airways. The trachea
leads the air into the bronchi, where flow is divided several times
before reaching the alveoli, where oxygen is supposed to enter
circulation. Since air flows within the different regions of the
pulmonary system are completely different, \gls{AP} is also handled
differently among them. Moreover, it is also important to consider that
pollutants also vary according to their own physical properties, and
pollutant absorption is also a function of this.  Particles' absorption
depends on their aerodynamic characteristics, as well as soluble
fraction and density. Gaseous pollutants are dependent exclusively on
their vapor pressure, solubility and density~\cite{Nilsson2010,
Vallero2014}.

\begin{figure}[htpb]
    \centering
    \includegraphics[clip,%
    trim=9.1cm 17cm 2.1cm 5cm,%
    width=.8\textwidth]{img/pdf/respiratory_system.pdf}
    \caption{Annotated anatomy of the respiratory
    system~\cite{Vallero2014}.}
    \label{fig:respiratory_system}
\end{figure}

The respiratory system has several (imperfect) mechanisms in place to
prevent particles from reaching the blood stream. Larger particles are
deposited in the nose, by impaction on the hairs and bends of the nose.
Smaller particles are immune to this first barrier, and manage to get to
the trachea and bronchi, where they are filtered also by impaction, this
time on the walls of the innumerable bifurcations of the bronchial tree.
The smallest particles are removed through Brownian motion, which ends
up pushing them against the alveolar membrane. Deposited substances are
then removed through the action of cilia in the pulmonary system's walls
or by coughing, sneezing or blowing one's nose~\cite{Vallero2014}.

While the body is quite efficient at filtering out particles from the
respiration process, the same cannot be said about gaseous pollutants.
Removal of these compounds can only be achieved through absorption,
which depends almost exclusively in the gases' solubility. High
solubility compounds are absorbed directly in the upper airways
(SO\textsubscript{2}, for instance), while less soluble gases (such as
O\textsubscript{3} and NO\textsubscript{2}) are absorbed in the lungs
themselves. Irritant gases trigger a variety of responses, in which one
can include sneezing, coughing or bronchoconstriction. These gaseous
compounds are then diffused through to the bloodstream or the lungs
themselves try to convert them into other substances via biochemical
processes. In some cases, this attempt to detoxify a pollutant can lead
to much more problematic circumstances. For instance, the lung is known
to active procarcinogenics, substances that are only carcinogenic after
being metabolized in a certain way~\cite{Vallero2014}.

Acute symptoms of \gls{AP} exposure are very varied, and range from mild
irritation to complete respiratory failure, depending mostly on level of
exposure and individual sensitivity to the chemical compound. One of the
most important acute manifestations of \gls{AP} exposure are encompassed
within the \gls{ALRI} group. There are several studies in which the
relationship between this issue and \gls{AP} is deducted and explained,
mostly in developing countries, and it remains as one of the major
causes for infantile death~\cite{CABI2019, UNICEF2013}. Children are one
of the most affected demographics by \gls{AP}~\cite{EEA2016}, and one of
the chief reasons for this is that the human respiratory system is till
developing in this stage of life.

In a 2016 review~\cite{Goldizen2016}, the authors searched the
literature for childhood adverse effects of \gls{AP}, with a particular
focus on respiratory problems. They have found evidence for a number of
respiratory complications and diseases that were previously reported in
the literature caused or exacerbated by \gls{AP}. Effects are many, and
vary immensely in nature, severity and affected populations. Short term
effects, like coughing and wheezing were found for the three types of
major pollutant and several others; several papers mention an
association between the occurrence of respiratory infections and
exposure to \gls{AP}, namely concerning \gls{PM} and \gls{no2}.  The
same review found reports of decreased lung function in children and
asthma exacerbation in children due to \acrlong{AP}. Moreover, a person
exposed to high levels of \gls{AP} during childhood are also more likely
to develop syndromes like \gls{COPD}, and to have exacerbated symptoms
of this disease. Finally, and perhaps more concerning, the carcinogenic
nature of several of the constituents of \gls{AP} leads to findings
relating the appearance of respiratory cancers to exposure levels during
childhood. Many of the conclusions of this review come from a
large-scale European effort called \gls{escape}, that intended to
investigate long-term health effects of \gls{AP} in Europe. \gls{escape}
was an \gls{fp7} initiative that ended in 2014.

\subsubsection{\acrlong{AP} and cardiovascular issues}%
\label{ssub:ap_and_cardiovascular_issues}

After being absorbed by the respiratory system, oxygen is distributed to
all cells of the body through the cardiovascular system. Air pollutants,
like particles and trace gases, are also capable of penetrating the lung
barrier and therefore share the same fate. There are several pathways
with which \gls{AP} and negatively affect the cardiovascular system. The
most immediate of which is probably an imbalance in the \gls{ans} caused
by direct inflammation and oxidative stress in the respiratory system.
The second most immediate pathway is systemic inflammation caused by
\acrlong{AP}. Finally, soluble \gls{AP} compounds in the bloodstream
also contribute to \gls{cvd} by increasing inflammation and oxidative
stress in the cardiovascular system~\cite{Brook2008, Vallero2014}.

The link between Air Pollution and cardiovascular effects started being
made during the twentieth century, given a series of incidents (like
London's 1952 killer-smog) that happened in the urban areas of
industrialized countries. Nowadays, \gls{cvm} has been shown to be
intricately connected to \gls{AP}. In fact, in a 2013 review indicated
that an annual increase of 10$\mu g / m^{3}$ in fine \gls{PM} and
NO\textsubscript{2} led to an increase of 11\% and 13\% respectively in
terms of \gls{cvm} and premature atherosclerosis, in spite of absolute
\gls{AP} concentrations were maintained below the European
policy-recommended thresholds. Road traffic exposure studies have
reported similar findings, with subjects having increased coronary
calcium scores~\cite{Bourdrel2017}.

Arrhythmia is one of the other cardiovascular issues that might be
caused by \gls{AP}. There is still some debate regarding whether or not
there is a causal relationship between the two, but there have been
several studies in which increased levels of \acrlong{AP} were
correlated with arrhythmia-related hospital admissions. Moreover, there
seems to be a correlation between low heart rate variability and
\gls{AP}, which is considered a marker for \gls{ans} imbalance and an
important risk factor for \gls{cvm}\cite{Bourdrel2017}.

The risk of stroke is also clearly exacerbated by the presence of
increased levels of \gls{AP}. In fact, it is currently thought that
\gls{AP} is responsible for about 29\% of the burden of stroke,
globally. Studies have shown that an increase of 5 $\mu g / m^{3}$ in
the annual \gls{PM}\textsubscript{2.5} concentration leads to a
remarkable 19\% increase in the risk of stroke, which was found to be
more significant in non-smokers. A positive correlation was also found
between gaseous pollutants (no\textsubscript{2}, CO and
SO\textsubscript{2}) concentration and the risk of stroke or stroke
mortality.

Short term effects of \gls{AP} on the cardiovascular system seem to be
predominantly the triggering acute coronary incidents. For instance, a
positive correlation was found between short term increases in \gls{AP}
and non-fatal myocardial infarctions.

\subsubsection{Gestational and developmental complications}%
\label{ssub:gestational_and_developmental_complications}

Mammals are in their life's most vulnerable stage while they are still
developing inside their mother's womb. This is the time when there is a
greater rate of tissue expansion and creation, creating an enormous need
for nutrients. These are supplied by the mother's blood, crossing the
placenta and reaching the fetus through its umbilical cord. High rates
of tissue formation and proliferation render the forming being unstable
and therefore more susceptible to the appearance of some kind of
morphological abnormality. At this time, there is no separation between
the mother's blood and the fetus, meaning that whatever chemical reaches
the progenitor's bloodstream also reaches the growing fetus. If the
mother is exposed, so is the fetus~\cite{Vallero2014}.

There are numerous chemicals that can affect the female reproductive
system, of which some are habitual components of \gls{AP}. They have
been associated to several highly adverse affects, and interfere with
such things as the processes by which the body is able to produce eggs,
or other processes that enable the formation of a single cell by the
union of the sperm and the egg (the zygote). After conception, \gls{AP}
has been known to reduce uterine nurturing capabilities, and hinder the
new being's development. Some of them are even teratogens, meaning that
they induce birth defects. Figure~\ref{fig:AP_in_utero} illustrates the
kind of defects that come with exposure, according to the time at which
the mother was exposed.

\begin{figure}[htpb]
    \centering
    \includegraphics[clip,%
        trim=3.4cm 12.2cm 3.4cm 3.6cm,%
        width=0.8\textwidth]{img/pdf/development.pdf}
    \caption{Possible abnormalities caused by \gls{AP} exposure \emph{in
    utero}. Notice that time of exposure is of critical
    importance~\cite{Vallero2014}.}
    \label{fig:AP_in_utero}
\end{figure}

There are already several studies that correlate higher \gls{AP}
exposure levels to birth defects or the probability of negative
outcomes. For instance, in~\cite{Li2019}, researchers have studied the
association between \gls{AP} exposure levels (for the mother) and the
appearance of premature \gls{sga} by collecting more than 40000 births
in Changzhou Maternity (China) and studying the mother's typical
environment. This study has found a positive association between
\gls{sga} and exposure to \gls{PM}\textsubscript{2.5} in two or three
pollutants models of \gls{AP} (with \gls{no2} and \gls{so2}), during the
third trimester of gestation. Another, perhaps more comprehensive study,
was performed using Swedish data from 1997 to 2007, and found that there
was a positive association between \gls{o3} exposure and the appearance
of pre-eclampsia (a potentially deadly complication of pregnancy),
estimating that about 1 in 20 pre-eclampsia cases were caused by
\gls{AP}~\cite{Olsson2013}.

Besides uterine development compromises, birth defects and reproductive
difficulties, \acrlong{AP} has also been associated with hindrances to
the child's neurodevelopment. In a New York study was able to associate
lower levels of mental development at age 3, in African-American
children with valid prenatal \gls{pah} exposure data. In another study
from the neighboring Boston, \gls{AP} was associated with generally lower
cognitive test scores, even when correcting for several influencing
factors. On a different level, \gls{AP} was shown to produce significant
delays in the central conduction times of \gls{baep} tests in children,
indicating that there might be important repercussions of \gls{AP} to
vestibular and auditory development.

Although most other systems are affected by \gls{AP}, it does have a
particularly heavy toll on the respiratory development. This is because
the lungs are not completely developed at birth, and are only finished
in the late teens. The level to which \gls{AP} affects the respiratory
system development varies greatly with the staeg of life in which the
effect is produced, and severity is also very varied. Acute negative
effects range from respiratory death to chronic
cough~\cite{Vallero2014}. Moreover, childhood (and prenatal) exposure to
\gls{AP} has been associated with the emergence of conditions such as
\gls{COPD} and asthma.

\begin{figure}[htpb]
    \centering
    \includegraphics[clip,%
        trim=3.4cm 15cm 3.4cm 3.6cm,%
        width=0.8\textwidth]{img/pdf/lung_development.pdf}
    \caption{Developmental stages of the lung throughout life vs the
    risks of \gls{AP} exposure in each stage~\cite{Vallero2014}.}
    \label{fig:img/lung_development}
\end{figure}

\subsubsection{Neurological disorders}%
\label{ssub:neurological_disorders}

The brain and the \gls{cns} were one of the last to be included in the
range of organs that are affected by \gls{AP}. While the effects of
\gls{AP} on the respiratory and cardiovascular systems are quite broad
and include some "surprises", the fact that these systems were affected
by \acrlong{AP} was evident and expectable, given the type of exposure
these systems endure. The \gls{cns}, on the other hand, has a more
difficult to express relationship with \gls{AP}, and has required more
sophisticated methods to detect~\cite{Vallero2014, Genc2012}. It was in
the beginning of this century that the first connections between
\gls{AP} and the emergence of neurological disorders started to be made,
and from then on, we have progressed into thinking that not only are
they related, but also that \gls{AP} might be one of the key driving
forces in the onset of certain neurological diseases, including the most
dreaded of them all, Alzheimer's and Parkinson's~\cite{Genc2012,
DePradoBert2018, Calderon-Garciduenas2014}.

The reason why \gls{AP} is able to reach and damage the \gls{cns} is a
continuation (or even an extension) of the ways in which it affects the
cardiovascular system. By crossing the alveolar barrier into the
bloodstream, \gls{AP} acts as an oxidative stress source. As it can also
do in lung tissue, \acrlong{AP} creates some local proinflammatory
effects in the cardiovascular system, affecting the vascular endothelium
cells. This can lead to a systemic inflammatory status, which is
accompanied by the production of proinflammatory cytokines (a type of
message-protein that is used by organisms to trigger certain types of
response, like inflammation~\cite{Zhang2007}). Now, since blood vessels in the
brain are extremely responsive to this kind of message, their presence
can activate cerebral endothelial cells and disrupt the blood-brain
barrier~\cite{Genc2012}.

In 2018, a consortium of several Spanish universities and researchers
wrote a review detailing the until-then-published articles dealing with
the neurological implications of \gls{AP}~\cite{DePradoBert2018}. This
review identifies several articles that connect the long-term exposure
to \acrlong{AP} with adverse impacts on the brain and brain structures.
\emph{In vitro} and \emph{in vivo} studies,  focusing on traffic related
emissions and their effect on gray matter cells, have found that these
display significant alterations. On other studies identified by the
review, it was shown that white matter, the myelinated part of the
brain, is particularly sensitive to \gls{AP} and its volume is
significantly decreased both in the elderly and children, as consequence
of prolonged exposure to it. 

There are also several articles that show that there is an association
between exposure to air pollutants and impairments on brain function.
In Section~\ref{ssub:gestational_and_developmental_complications},  I
have already mentioned a study that was conducted in New York, and that
found that the children that they were using as subjects were found to
have measurable cognitive deficits in comparison with children of the
same age living in less polluted areas which are compatible with the
affected areas of the brain that were detected through neuroimaging
studies~\cite{DePradoBert2018}.

\subsection{\acrlong{AP} effects on ecosystems}%
\label{sub:ap_effects_on_ecosystems}
\todo[inline]{citations - vallero and lovett + european report}
The Earth is home to an almost unbelievable number of different
ecosystems. The ubiquitousness of \gls{AP} means that all of them are in
some way or another affected by this problem. In general terms, the
threat posed by \gls{AP} to any given habitat is a function of its
biodiversity, defined as the number of different living beings that
inhabit a certain environment (in all biological
kingdoms)~\cite{OxfordPress2020}. Living beings within an ecosystem are
like nodes in a graph, with many connections to any particular node.
More biodiversity corresponds to a greater number of nodes and an even
larger number of links,  which means that there is a greater probability
that some of those links become disrupted by \acrlong{AP} in some way.

Water based environments are greatly affected by \gls{AP}. Material
deposition on the surface of the water can have serious consequences in
terms of habitat conditions for holding life. In this regard, the most
important air pollutants are \gls{no2} and \gls{so2},  which
significantly decreases the water's pH. On its own, this represents a
major problem. The acidifying effects of nitrogen and sulfur deposition
became very pronounced in Scandinavia (among other places). Thousands of
this territory's lakes, once teeming with wildlife, became effectively
lifeless. Those that did not reach this point, have seen the number of
fish living on their waters dwindle to numbers from which there may be
no return~\cite{DeWit2015}. Sulfur and nitrogen depositions also enrich
surface waters, altering the solubility and other physical aspects on
the surface of the water, which in turn inevitably leads to disruptions
in species abundance and diversity. Moreover, indirect effects may also
take their tolls. For instance, \gls{o3} does not play any significant
role in the chemical behavior of a water body, but it can influence the
number of predators around this habitat, which will compromise the
predator-prey balance of the aquatic environment~\cite{Vallero2014,
Lovett2009}.

In terrestrial ecosystems, \gls{AP} effects are not smaller in
importance or complexity, and they are different for each type of being.
To the Flora, \gls{AP} can have a subtle to deadly effect, depending on
variables like pollutant chemical species, exposure time, or plant life
stage in which exposure happens. For instance, \gls{o3} is especially
poisonous to plants. Even small concentrations of this gas will cause
plant growth to decrease significantly. It enters the plant through the
stomata and reduces photosynthesis through increased oxidative stress.
Many times, although concentrations are not enough to outright kill the
plant, they are enough to make them more susceptible to other attacks
like pathogens, insects or environmental conditions. \acrlong{o3} is
commonly responsible for huge financial losses that come from the
diminished agricultural yields. And while it is true that due to several
policies, \gls{AP} is in a clear downward trend since the 1980s in urban
regions, it is also true that in many rural areas, these changes have
been smaller or non-existent, making these losses even more relevant.

Forests are among the most susceptible environments to \gls{AP}. They
suffer from the previously described mechanisms of \gls{AP} damage, like
acidic deposition, but also suffer from other, less direct pollution
risks. Emission of greenhouse gases can induce changes in humidity,
temperature, and general climate profile of a forest. The combination of
direct and indirect risks result in an exacerbation of both, leading to
more and more forest losses due to \acrlong{AP}. The damage done to
forests all around the world is especially problematic given the
biodiversity that these ecosystems contain within themselves.
Rainforests in particular are thought to contain more than half of the
world's terrestrial species. These species have many times adapted to a
particular kind of microhabitat which only exists in the specific
rainforest in which it lives. Changes in these specific conditions,
whether caused by \acrlong{AP} or any other cause, are leading to
alarming extinction rates in forests and rainforests globally.

Of course it is not only the flora that suffers with \acrlong{AP}.
Direct implications of \gls{AP} on animals approximate those that fall
upon humans. We are an animal species, after all. Our main difference is
the adaptation capabilities that our superior intellect grants us, which
allows us to escape more or less unscathed for a longer period of time,
and to combat what we cannot escape from in ways which are simply
unaccessible to other animal species. So, although \gls{AP} has direct
effects on all animals that are exposed to it, ecosystem damage and
eventual destruction remains the most perilous factor for this
biological realm~\todo{should I get another source on this problem? }.





\subsection{\acrlong{AP} Sources}%
\label{sub:ap_sources}

There are almost as many \gls{AP} sources as there are pollutants. The
first major division between these sources is whether they are natural
or anthropogenic. However, this separation is not always clear, as one
source can lead to another and boundaries become fuzzy within their own
context. The most prominent example of such is the case of accidental
fires. While they are most of the times classified as a natural source
of \gls{AP}, their origin lies most of the times in human activities. In
this section, I will present a selection of the most important naturally
occurring air pollutants and examples of how they have affected human
lives throughout the times. The selection itself does not intend to be
complete description of pollution sources, but rather paint a general
picture of the subject.

\subsubsection{Natural Sources of \acrlong{AP}}%
\label{ssub:natural_sources_of_ap}

Although people, governments and institutions tend to speak far more
seldomly of them than of their man-made counterparts, natural sources of
air pollutant are not only abundant, but also important. One of the main
natural sources of \gls{AP} are volcanic eruptions. These phenomena are
responsible for the emission of immense quantities of \gls{PM} and gases
such as \gls{so2}, \gls{h2s} and methane. Depending on the type of
volcanic eruption, the emitted cloud of gas and \gls{PM} can remain
airborne for long periods of time, even disrupting modern life at times,
namely in what concerns air travel. The last eruption to happen in
Portuguese soil took place in the remote Azorian island of Faial. In
September 1957, the Earth shook almost continuously for around two
weeks. Finally, on the 27\textsuperscript{th}, 100 m Northeast the
Capelinhos islands, the sea was seen to boil and project vapor and
volcanic material hundreds of meters into the air. In the following
hours, the underwater volcano finally exploded, emitting large
quantities of volcanic ash and gases into the atmosphere. The phenomenon
lasted for more than a year, and the final ejection of lava took place
in October 1958 (see Figure~\ref{fig:capelinhos1957}). The eruption had
a significant social impact, in addition to its ecologic importance. In
the end, 40\% of Faial's population left the island as a
result~\cite{Vallero2014, TSF2017}.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.8\linewidth]{img/jpg/capelinhos1957.jpeg}
    \caption{Dramatic photograph depicting the Capelinhos' lighthouse,
    half a kilometer from the eruption site, surrounded by a cloud of
    ash \gls{PM}, volcanic gas and water vapor with more than 1km in
    height\cite{TSF2017}.}
    \label{fig:capelinhos1957}
\end{figure}

Oceans are also a significant source of \gls{AP}. Aerosol particles of
salt are continuously emitted from these large masses of salt water,
which damage many human created structures, namely metallic
constructions. In certain parts of the world, another important source
of \acrlong{PM} (especially because of its consequences in the
inhabitants' daily life) are dust storms. The most famous of these
events, and one of the most deadly storms in the recorded history of the
US territory was the infamous \emph{Black Sunday} dust storm. Starting
on Palm Sunday, 14 April, this sky-blackening dust storm punished the
peoples from the panhandles of Texas and Oklahoma, burying entire houses
(see Figure~\ref{fig:dust1935}) under the dust and destroying the
livelihoods of thousands of Americans. Dust storms were an important
part of the US history during the 1930s and led to the creation of the
Soil Conservation Service, a branch of the US Department of
Agriculture~\cite{Vallero2014,Agriculture2012, Reis2008}.


\begin{figure}[htpb]
    \centering
    \includegraphics[clip,%
        trim=4.3cm 15cm 2cm 3.6cm,%
        width=0.8\textwidth]{img/pdf/dust.pdf}
        \caption{House almost completely buried by the Black Sunday dust
            storm. Several houses were entirely swallowed during this
            storm, trapping people inside, as if a big blizzard had hit
            them. Unlike a blizzard though, there was nothing anyone
            could do to keep the dirt outside, and all surfaces were
            covered black~\cite{Reis2008}.}
    \label{fig:dust1935}
\end{figure}

Fires are also one of the largest sources of natural air pollution in
the world. The uncontrolled burning of organic matter that is a large
forest fire creates a large quantity of air pollutants that range from
smoke to unburned (or partially burned) hydrocarbons, nitrogen and carbon
oxides, and ash particles. Besides the obvious dangers of this kind of
burnings for human life and activities, forest fires can also cause
indirect damages, such as disruptions in supplies and travel due to
reduced visibility~\cite{Vallero2014}.

Trees and forests in themselves are also responsible for a certain
quantity of air pollution. Although they have the main part in the
carbon dioxide conversion into oxygen, through photosynthesis, plants
and trees are still the largest emitters of hydrocarbons in the planet,
as attested by the blue haze that is visible on top heavily forested
areas, resulting from chemical reactions between \gls{VOC}s produced by
the trees. This counter-intuitive fact was in the origin of the infamous
Ronald Reagan speech in which he "blamed" trees for much of \gls{AP}, in
a time when anthropogenic \gls{AP} was at its apogee in the US and
Europe.Plants are also the emitters of another kind of \gls{PM}, which
is of particular importance both to themselves and humans, which are the
pollens. This is a bio-aerosol - a type of aerosol that is or was part
of a living being - associated with a number of
diseases~\cite{Vallero2014}.

Finally, I will discuss Radon gas. This is a natural occurring
radioactive gas that is part of the radiative decay of Uranium present
in all rocks. Although chemically inert, Radon is radioactive and, as
all radioactive substances, emits particles when it decays. Although
present virtually everywhere, outdoor concentrations of Radon are
typically too small to cause any problems. The problem with this gas
comes essentially from indoor concentrations, namely at home. Being a
gas, Radon is able to enter people's houses, exposing the inhabitants.
Prolonged exposure to Radon gas is the second biggest cause of lung
cancer and authorities estimate that between 3 and 14\% of lung cancer
cases are caused by this gas. In Portugal, Radon concentrations were
found to be below the European prescribed limit in two thirds of the
houses in a 2001 study, but in 17\% of the cases, concentrations were
not only above this limit, but also over the highest tolerable
limit~\cite{Vallero2014, WorldHealthOrganization2016, ProTeste2003}.


\subsubsection{Anthropogenic Sources of \acrlong{AP}}%
\label{ssub:anthropogenic_sources_of_ap}

\acrlong{AP} that originates from human activities is called
anthropogenic. Since the first industrial revolution, mankind has been
using more and more resources to fuel our progress and continuously
improving way of life. Of course, the consumption of natural resources
has some unpleasant and sometimes dangerous consequences. The most
important of which, looking from the lens of this thesis, is the
incredible increase in the levels of \gls{AP}. If one had any doubts
whatsoever, all it would take would be a look into the atmospheric
\gls{co2} concentration chart (Figure~\ref{fig:co2_concentration}) from
a few centuries back to the current day to completely dissipate them.

\begin{figure}[htpb]
    \centering
    \includegraphics[clip, % left, bottom, right, top
                     trim=0cm 1cm 0cm 4.5cm,%
                     width=0.8\linewidth]{img/pdf/co2_ice_cores.pdf}
                     \caption{\gls{co2} atmospheric concentrations since
                         the year 1000. Note the seemingly exponential
                         increase since the 1800s. Plotted and published
                         by the 2 Degrees Institute~\cite{co2levels2020}
                         with data from ice cores~\cite{Etheridge} and
                         in situ monitors~\cite{Tans}.}
    \label{fig:co2_concentration}
\end{figure}

There are literally hundreds of sources of \gls{AP}, but it is possible
to categorize them into 4 main \emph{families}: industrial processes,
energy (includes transportation), agriculture and forestry, and waste.
Of these 4 broad categories, as displayed in
Figure~\ref{fig:anthropogenic_pollution_categories}. The most prominent
is without a doubt the energy sector, although we also have to bear in
mind that any and all combustion used in the other sectors is counted as
energy production~\cite{InternationalAgencyforResearchonCancer2016,
CABI2019}.\todo{table with major pollutants?}

\begin{figure}[htpb]
    \centering
    \includegraphics[clip,% left, bottom, right, top
        trim=2.3cm 7.3cm 2.3cm 2.6cm,%
        width=\textwidth]{img/pdf/pollution_sources.pdf}
    \caption{Schematic presentation on the sources of anthropogenic
    pollution and its categorisation according to the IPCC. Adapted
    from~\cite{CABI2019}}
    \label{fig:anthropogenic_pollution_categories}
\end{figure}

From 2002 to 2011, fossil fuel combustion has been responsible for an
average of 8.3 petagrams of carbon per year. This truly gigantic carbon
footprint is in its majority explained by the worlds energy needs, which
are ever increasing up to now. In 1990, total energy demand was situated
at 356 quadrillion \gls{btu}, having grown to 410 quadrillion \gls{btu}
in 2010. In 2020, energy demand estimates are located at 600 quadrillion
\gls{btu}, of which almost a quarter was expended by
China~\cite{CABI2019}.

It is important that we focus a little bit more on the Chinese case. It
is now somewhat near commonsense to regard China as the factory of the
world, and this of course is tied to Chinese energy consumption and
production. On the same line of reasoning, this must mean that in some
way, the country's energy expenditure is connected to the amount of
financial resources that it produces, the \gls{gdp}. Looking at the
plots in Figure~\ref{fig:china_energy}, one can see that all these
numbers are highly correlated. When we ponder on the case of Chinese
\gls{AP}, and wonder why has this problem not been addressed previously,
given its imposing dimensions and growing importance, one must take into
account that, given the indirect importance of \gls{AP} on Chinese
people's gains, it is highly likely that the country's governments will
be reluctant to decrease it in any expedient form~\cite{CABI2019, IEA,
WorldBank}.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=\linewidth]{img/eps/energy_co2_gdp.eps}
    \caption{Chinese energy production, \gls{gdp} and \gls{co2}
    emissions.  Data collected from the World Bank and International
    Energy Agency websites~\cite{WorldBank, IEA}}
    \label{fig:china_energy}
\end{figure}

Another important conclusion that we can take from the plot in
Figure~\ref{fig:china_energy} is that China has a large and historical
dependence on the use of coal as fuel for energy production. This adds
to the problem described in the above paragraphs, as coal is the single
most damaging fossil fuel available. Not only does China get most of its
energy from coal burning, but is also responsible for more than half of
the worlds production and consumption of this substance (see
Table~\ref{tab:global_energy_consumption}).


\begin{table}[htpb]
    \centering
    \small
    \caption{Global energy production, divided according to the fuel
    used to obtain it and the production country.}
    \label{tab:global_energy_consumption}
    \begin{tabularx}{\textwidth}{lXXXXX}
    \toprule
    \textbf{Country} & \textbf{Liq. Fuel} &
    \textbf{Coal} & \textbf{Nat. Gas} &
    \textbf{Renew.} & \textbf{Nuc.} \\ 
                    &\textbf{\tiny{(M barrels /
    day)}}&\textbf{\tiny{(BTU)}}&\textbf{\tiny{(T cu.
                    ft)}}&\textbf{\tiny{(BTU)}}&\textbf{\tiny{(b kWh)}}\\\midrule
    \textbf{China} & 10,6 & 80,6 & 5,1 & 10,6 & 93 \\
    \textbf{USA} & 18,5 & 17,3 & 25,5 & 7,8 & 769 \\
    \textbf{Europe} & 14,4 & 12,5 & 17,9 & 11,7 & 837 \\
    \textbf{Middle East} & 16 & 0,1 & 14,8 & 0,2 & 1 \\
    \textbf{India} & 3,6 & 12,6 & 2,1 & 3,5 & 30 \\
    \textbf{Russia} & 3,4 & 4,5 & 15,7 & 1,7 & 166 \\
    \textbf{Africa} & 3,5 & 4,3 & 2,7 & 4,7 & 12 \\
    \textbf{Brazil} & 3,3 & 0,5 & 1,1 & 6,8 & 15 \\
    \textbf{World} & 91,4 & 153,9 & 120,8 & 63,7 & 2345 \\ \bottomrule
    \end{tabularx}
\end{table}

\gls{ice} are the single most important means for powering human
transportation. Almost every vehicle in the world uses a kind of
\gls{ice}. These motors operation is an application of the Otto cycle,
in which the chemical energy in the fuel is converted to mechanical
energy. These engines are as ubiquitous as the fossil fuels that have
powered them since the beginning of the automobile revolution, in the
early 20\textsuperscript{th} century. Fossil fuels have several features
that make them ideal to power our vehicles. Their energy density is
generally high~\todo{citation and examples}, they are incredibly safe to
manipulate and use, and fossil fuel infrastructure can be found in
almost every far corner of the Earth. However, using them releases a
number of gaseous and particle-condensed side products into the
atmosphere, and this makes traffic one of the most important sources of
\gls{AP}. For instance, traffic pollution is the main responsible for
human exposure to \gls{nox} gases. Without countermeasures, gasoline
\gls{ice} equipping passenger vehicles emit around 1.8 g/km of these
gases, while diesel emits 2.8 g/km and \gls{gpl} around 2.1 g/km. On
heavy duty engines, like on trucks and tractors, these figures skyrocket
to 14.7 g/km for diesel engines and around 5.1 g/km for
\gls{gpl}~\cite{CABI2019}.

Energy production (including transportation) is clearly the single
largest contributor to global \gls{AP}. This does not mean that other
human activities do not pollute or produce air pollutants. Pollutant
contributions from the industry, the agricultural activities and waste
disposal are also non-negligible. In fact, industries around the world
are responsible for the production and emission of all the criteria
pollutants. It is important to single out one particular activity, which
is the burning of forest for land-use changes. \gls{co} emissions for
this purpose are very high due to the nature of the burning material,
which emits more than 50 times more \gls{co} than fuel or
coal~\cite{CABI2019}.

\subsubsection{The European Case}%
\label{ssub:the_european_case}

Europe has for long been on the forefront of the fight against \gls{AP}
emissions. The European Union has put in place a number of policies
aiming at cutting (or even eliminating) emissions of human health
compromising pollutant components. Few places in the world have been so
demanding regarding their environmental practices, and numbers are a
clear reflection of these adaptation efforts. In their 2019 report, the
\gls{EEA} state that European emissions have globally declined, and have
been declining since at least the year 2000. Moreover, and in contrast
with China's case, the \gls{gdp} does no seem to be connected to
\gls{AP} emissions. As can be seen in
Figure~\ref{fig:european_emission_trends}, emissions are decoupled from
economic growth, as there are now less emissions per \gls{gdp} unit than
before~\cite{EEA2019}.

\begin{figure}[htpb]
    \centering
    \includegraphics[clip,% left, bottom, right, top
        trim=0cm 15cm 0cm 5.8cm,%
        width=\textwidth]{img/pdf/european_emission_trends.pdf}
    \caption{General trends for European emissions. Data presented in \%
    emissions of year 2000. Note the downward global trend in pollutant
    emissions, and its decoupling with the European GDP~\cite{EEA2019}.}
    \label{fig:european_emission_trends}
\end{figure}

If one extends this analysis further, and separates emissions by using
their origin, the trends are approximately the same: except for
\gls{nh3} (a side product of agricultural activities) a clear reduction
is present in all sectors. These results can be seen in
Figure~\ref{fig:european_emissions_by_sector} and were presented
in~\cite{EEA2019}.

\begin{figure}[htpb]
    \centering
    \includegraphics[clip,% left, bottom, right, top
        trim=0cm 6.5cm 0cm 5.3cm,%
        width=\textwidth]{img/pdf/european_emissions_by_sector.pdf}
    \caption{European emissions divided by activity sector. The global
    decreasing trend is confirmed, as industries all around are
    producing less and less \gls{AP} with the passing years~\cite{EEA2019}.}
    \label{fig:european_emissions_by_sector}
\end{figure}

\subsection{Detecting and Monitoring \acrlong{AP}}%
\label{sub:detecting_and_monitoring_ap}

There is no doubt that \gls{AP} is a global threat that affects
everyone, both in personal terms (through the degradation of their
health) and in societal terms, through the investments and limitations
that we as a whole have to commit to in order to prevent larger,
unmanageable problems. Reducing \gls{AP} is a priority and a requirement
for today's modern societies. This demands immediate and effective
actions, which in turn imply that we have a solid and profound
understanding of how pollutants are created, transported and transformed
in the atmosphere. The scale on which these interventions must be
conducted requires them to be made on a concerted and collaborative
manner, and always leveraged by technological
development~\cite{EEA2019}. Many of the air pollutants cannot be
detected solely by our senses, or even if they can is at already
dangerous concentrations. Technology is therefore a prerequisite to our
fighting the problem of \acrlong{AP}~\cite{Vallero2014}.

Pollution monitoring is itself based on the ability of a given
measurement method to determine concentrations for trace gases, aerosols
or radiation quantities. As with many other test techniques, in various
fields, pollution monitoring techniques have three very important
aspects to verify. The first of which is sensitivity, and also the most
demanding. Important trace gases in atmospheric chemistry have sometimes
vestigial concentrations, and the ability to correctly detect them is
many times a technical challenge. The second most important is
specificity, which is the ability of an atmospheric measurement to
measure each compound independently, without a component influencing
another component's measurement either positively or negatively.
Finally, any usable monitoring technique must be sufficiently precise as
to provide valid measurements.

\acrlong{AP} monitoring techniques and devices are too many to address
them all in this document. Besides, the physical principles involved are
completely different from one to another, making it very difficult to
make a broad generalization, other than the fact that they can be
divided into local and remote sensing devices. The gold standard for air
quality measurements remains those techniques in which a sample is
collected in the field and then taken to the laboratory to be analyzed
by very powerful analytical methods such as chromatography or mass
spectroscopy. While undoubtedly providing the most accurate
representation of the air composition at the time and place the sample
was collected, it is also true that this method's results are too slow
to use regularly in the field.

Another very important air quality monitoring method is the use of
electrochemical sensors. The first variants (wet cells) of this kind of
sensor became very popular in the field of industrial hygiene, where
they were applied in many portable flue gas analyzers. They were very
attractive to companies worldwide given their potential for very low
costs in comparison to optical or other more complex techniques. Apart
from the oxygen wet cell sensor, that has a slightly different
configuration, these electrochemical devices are comprised of three
electrodes - a sensing electrode, a counter electrode and a reference
electrode - separated by a thin layer of electrolyte. The gas that is
diffused to the surface of the electrode is either oxidized or reduced,
thus changing the systems electrical properties, in a way that is then
captured by an amplification circuit.

Wet cell electrochemical sensors were the precursors of the now more
common solid state sensors. These sensors are the ones that we see in
every subterranean parking lot, measuring several traffic related gases
such as \gls{co}, \gls{co2} or \gls{no2}. In general semiconductor gas
detectors are comprised of two modules: a receptor and a transducer (see
Figure~\ref{fig:semiconductor_sensor}). The receptor has in its
composition a material (or set of materials) that, in contact with the
target gas, induces a change in the systems inherent properties (work
function, dielectric constant, resistance, etc.) or emits heat or light.
The transducer is a device or circuit that converts the receptor's
changes into an electrical signal. There are 5 types of semiconductor
sensors, according to the material from which the transducer is made.
These types are enumerated in Table~\ref{tab:semiconductor_sensor}.

\begin{figure}[htpb]
    \centering
    \missingfigure{}
    \caption{Semiconductor electrochemical sensor basic structure. There
    are many examples of this type of sensors, but in general they
    follow this architecture.}
    \label{fig:semiconductor_sensor}
\end{figure}

\begin{table}[htpb]
    \centering
    \caption{Categorization of semiconductor gas sensors. The type of
    transducer and receptor dictates the type of the sensor.}
    \label{tab:semiconductor_sensor}
    \missingfigure{This is actually a table}
\end{table}

Optical (spectroscopic) systems are fundamentally different from the
other techniques that have already been presented. They can be used to
perform remote sensing measurements (as far as being used for
measurements aboard satellites). In the last few decades, these
techniques have gained a lot of ground in atmospheric research, due to
their high sensitivity and specificity and the universality of their
applicability. Spectroscopic methods are based on Lambert-Beer's law
(see Section~\ref{sec:doas} for a more thorough explanation), and make
use of the fact that the way in which gases interact with light is well
known and follows an exponential expression. There are many
spectroscopic techniques for measuring \gls{AP}. Selecting one requires
careful consideration of a number of factors like the target gases,
optical path arrangement, type of light source, etc.
Figure~\ref{fig:selecting_spectroscopic_method} provides several
examples of optical measurement methods, and how they can be divided.

\begin{figure}[htpb]
    \centering
    \missingfigure{}
    \caption{There are many spectroscopic techniques for atmospheric
    trace gas concentration measurement. Although several are depicted
    here, keep in mind that this is not an exhaustive list and is meant as
    an example repository.}
    \label{fig:selecting_spectroscopic_method}
\end{figure}

One of the most important atmospheric analysis methods, and especially
in what concerns this document, is \gls{DOAS}. While it is based on the
same mathematical formulation as the other spectroscopic methods, it is
also based on other factors, which shall be discussed in
Section~\ref{sec:doas}.

\section{DOAS}%
\label{sec:doas}

Differential Optical Absorption Spectroscopy is a well established
absorption technique that is widely used in the field of atmospheric
studies~\cite{Platt2007}. In this section, I present a short
introduction to the field, extracted from~\cite{ValentedeAlmeida2017},
an article we have published in 2017, marking the conclusion of the
initial studies for this PhD thesis.

There are two main families of \gls{DOAS} assemblies, with different
goals and capabilities:

\begin{itemize}

        \item Active systems, of which a simple illustration is
            presented in Fig.~\ref{fig:activeSmall}, are characterized
            by relying on an artificial light source for their
            measurements. A spectrometer at the end of the light path
            performs spectroscopic detection. Active DOAS techniques are
            very similar to traditional in-lab absorption spectroscopy
            techniques \cite{Platt2007};

        \item Passive DOAS techniques, illustrated in
            Fig.~\ref{fig:passiveSchematic}, use natural light sources,
            such as the Sun and the moon, in their measurement process.
            An optical system is pointed in certain elevation and
            azimuth angles and sends the captured light into a
            spectrometer, connected to a computer. The system returns
            the total value of the light absorption in its
            path~\cite{Platt2007,Merlaud2013}.

\end{itemize}

%f2
 \begin{figure*}[t]
    \includegraphics[width=14cm]{img/pdf/amt-2016-314-f02.pdf}
    \caption{Active DOAS schematic.}\label{fig:activeSmall}
  \end{figure*}

%f3
  \begin{figure*}[t]
      \includegraphics[width=14cm]{img/png/amt-2016-314-f03.png}
      \caption{Passive DOAS schematic.}\label{fig:passiveSchematic}
  \end{figure*}

DOAS itself is based on Lambert--Beer's law, which can be written as
\cite{Platt2007}

\begin{equation}
  \centering
  \label{eq:lambertBeer}
  I(\lambda) = I_0 (\lambda) \cdot \exp(-\sigma(\lambda) \cdot c \cdot L) \;,
\end{equation}

Where $\lambda$ is the wavelength of the emitted light; $I(\lambda)$ is
the light intensity as measured by the system; $I_{0}(\lambda)$ is the
intensity of the light as emitted by the source; and $\sigma(\lambda)$
is the absorption cross section of absorber, which is wavelength
dependent; $c$ is the concentration of the absorber we want to measure.


This law allows the definition of optical thickness
($\tau$)~\cite{Platt2007}:

\begin{equation}
      \label{eq:opticalThickness}
      \tau(\lambda) = \ln \bigg( \frac{I_{0}(\lambda)}{I(\lambda)}\bigg) = \sigma(\lambda) \cdot c \cdot
      L.
\end{equation}

In a laboratory setting, Eq.~(\ref{eq:lambertBeer})
or~(\ref{eq:opticalThickness}) can be used to directly calculate an
absorber's concentration, provided there is knowledge of  its cross
section. In the open atmosphere, however, absorption spectroscopy
techniques are far more complex. On one hand, $I_0(\lambda)$ is not
accessible since we measure from inside the medium we want to measure.
On the other hand, there are several environmental and instrumental
effects that influence measurement results. These effects include the
following~\cite{Platt2007}.

\begin{itemize}
      \item Rayleigh scattering is due to small molecules present in the
          atmosphere and is heavily influenced by wavelength (hence the
          blue colour of the
      sky).
      \item Mie scattering is caused by particles and larger molecules
          suspended in the atmosphere and is not very dependent
      on the wavelength (hence the white colour of clouds).
      \item Instrumental and turbulence effects are the instrument's
          transmissivity and atmospheric turbulence in the optical path
          also limit light intensity.

  \end{itemize}

In addition, we also have to take into account that, in the atmosphere,
there are a number of trace gases that interfere with passing light.

Another aspect worth mentioning is that our device is never pointed
directly at the light source (the Sun) but always processes light that
has been scattered at some unknown point in the optical path. This means
that the light that reaches our detector is only the scattered fraction
of the sunlight, depending on the system's position and geometry, as
well as wavelength.

The expansion of Lambert--Beer's equation to include all these effects
results in Eq.~(\ref{eq:expandedLambertBeer}).

\begin{align}
      \label{eq:expandedLambertBeer}
I(\lambda) & = I_{0}(\lambda) \cdot A(\lambda, \ldots) \cdot S(\lambda) \nonumber \\
      &\cdot
      \exp \Bigg[ - \int \Big[ \Big(\sum_{i} \sigma_{i}(\lambda, s) \cdot c_{i}(s)\Big) +
      \epsilon_\mathrm{M}(\lambda, s)\nonumber\\
      & + \epsilon_\mathrm{R}(\lambda, s) \Big]\mathrm{d}s \Bigg],
\end{align}

Where $A(\lambda, \ldots)$ is the fraction of scattered light that
reaches the device, $S(\lambda)$ represents instrumental and turbulence
effects, $\sigma_{i}(\lambda, s)$ is the absorption cross section of
absorber $i$, $c_{i}$ is the concentration of absorber $i$,
$\epsilon_\mathrm{R}(\lambda)$ represents Rayleigh's extinction
coefficient and $\epsilon_\mathrm{M}(\lambda)$ represents Mie's
extinction coefficient.


The interest of this equation lies within the retrieval of $c_i$, a
given absorber's concentration. Since the integral is taken along the
total atmospheric path of the measured photons, and considering that
their cross sections do not vary significantly in atmospheric
conditions, it is possible to define the concept of slant column, which
is of great importance~\cite{Merlaud2013}.

\begin{equation}
      \label{eq:slantColumn}
      \mathrm{SC}_{i} = \int c_{i}(s)\mathrm{d}s
\end{equation}

This quantity, as Eq.~(\ref{eq:slantColumn}) shows, equals the integral
of an individual absorber's concentration along the atmospheric optical
path of relevance.

Now, without knowledge of $I_{0}(\lambda)$, these equations cannot give
us absolute concentration values. We can, however, use another scattered
light spectrum as reference in Eq.~(\ref{eq:opticalThickness}). Instead
of absolute densities, this will yield relative changes in the
atmosphere. We thus arrive at Eq.~(\ref{eq:relativeOpticalThickness}).

\begin{align}
      \label{eq:relativeOpticalThickness}
      \ln\Big( \frac{I_\mathrm{ref}}{I}(\lambda) \Big) &= \ln\Big( \frac{A_\mathrm{ref}}{A}(\lambda,\ldots) \Big) + \ln\Big( \frac{S_\mathrm{ref}}{S}(\lambda) \Big) \nonumber\\
      &+  \sum_{i} (\sigma_{i}(\lambda) \cdot \Delta \mathrm{SC}_{i}(\lambda)) + \Delta \tau_\mathrm{M}(\lambda) \nonumber\\
      &+ \Delta \tau_\mathrm{R}(\lambda),
\end{align}

Where $\Delta \mathrm{SC}_{i}$  is the relative slant column of absorber
$i$; $\Delta \tau_\mathrm{M}$  is the relative Mie scattering term,
integrated to its optical thickness; and $\Delta \tau_\mathrm{R}$ is the
relative Rayleigh scattering term, integrated to its optical thickness.


This is where the principle of DOAS is applied. Instrument features,
scattering and other atmospheric effects have broad absorption spectral
profiles, which vary slowly with wavelength. Several trace absorbers
have narrow and rapidly varying spectral signatures in at least a small
section of the spectrum. By using Eq.~(\ref{eq:separation}), we can
separate these contributions \cite{Danckaert2015}.

\begin{equation}
      \label{eq:separation}
      \sigma(\lambda) = \sigma{'}(\lambda) + \sigma_{0}(\lambda)
\end{equation}

Here, the broad part of the optical thickness ($\sigma_{0}(\lambda)$)
can be separated from the narrow part ($\sigma{'}(\lambda)$ --
differential) by approximating it by a low-order polynomial, resulting
in Eq.~(\ref{eq:DOAS}).

\begin{equation}
      \label{eq:DOAS}
      \ln\Big( \frac{I_\mathrm{ref}}{I}(\lambda) \Big) = \sum_{i = 1}^{n} \sigma_{i}{'}(\lambda) \cdot \Delta \mathrm{SC}_{i} + \sum_{j = 0}^{m} a_{j} \cdot
      \lambda^{j},
\end{equation}

Where $\sum_{i = 1}^{n} \sigma_{i}{'}(\lambda) \cdot \Delta SC_{i}$ is
the differential part (narrowband, rapidly varying with wavelength) and
$\sum_{j = 0}^{m} a_{j} \cdot \lambda^{j}$ is a low-order polynomial,
used to remove the broadband spectral features resulting from
atmospheric and instrumental phenomena.


In practice, the mathematical solving of Eq.~(\ref{eq:DOAS}) is not
enough since it does not account for the Ring effect or the
non-linearities that result from stray light and wavelength shift in
measured and cross-section spectra.

The Ring effect is a consequence of rotational Raman scattering:
molecules in the atmosphere do not absorb photons in a purely elastic
(Rayleigh scattering) fashion. A small portion of the light--matter
interaction is in fact inelastic \cite{Brinkmann1968,Merlaud2013}. This
changes the light source frequencies as seen from the detector. This
phenomenon was first noticed by Grainger and Ring in 1962. At the time,
they noticed that the well-known Fraunhofer lines would slightly change
when one  observed them by using moonlight instead of scattered daylight
\cite{GRAINGER1962}.

From the occurrence of these phenomena, it results that the mathematical
procedure for DOAS measurements consists in solving a linear and a
non-linear problem. The linear problem is solved by writing
Eq.~(\ref{eq:DOAS}) in its matrix form:

\begin{equation}
      \label{eq:DOAS_matrix}
      \tau = \mathbf{A} \cdot X.
\end{equation}

$\mathbf{A}$ is an $m\,\times\,n$ matrix, with its columns being the
differential cross sections $\sigma_{i}{'}(\lambda)$ and the wavelength
powers taking the polynomial $P(\lambda) = \sum_{j = 0}^{m} a_{j} \cdot
\lambda^{j}$ into account. Since the number of lines in $A$ is much
larger than the number of columns, the system is overdetermined and, in
this case, we must use methods to numerically approximate a solution. It
is common to use the least-squares approach, in which the best solution
is the one that minimises $\chi^{2} = \left[\tau - A \cdot X\right]
\cdot \left[\tau - A \cdot X\right]^{T}$.

While the Ring effect is treated as a pseudo-absorber, a synthetically
produced~\cite{Chance1997} cross section that is fitted just like any
other absorber, non-linearities are addressed by applying
Levenberg--Marquardt's approach to non-linear fitting problems to
Eq.~(\ref{eq:DOAS_nonLinear}) \cite{Merlaud2013,Press2007}:


\begin{align}
      \label{eq:DOAS_nonLinear}
      &\ln\Big( \frac{I_\mathrm{ref}(\lambda)}{I(\lambda + \mathrm{shift}) + \mathrm{offset}} \Big) = \sum_{i = 1}^{n} \sigma_{i}{'}(\lambda) \cdot \Delta \mathrm{SC}_{i} \nonumber\\
      &+ \sum_{j = 0}^{m} a_{j} \cdot \lambda^{j},
\end{align}

Where shift and offset, which represent spectral wavelength shifts and
stray light offsets, respectively, are responsible for the non-linear
character of the problem.


\section{Tomographic algorithms and reconstruction techniques}%
\label{sec:tomographic_algorithms_and_reconstruction_techniques}

\subsection{Introduction}%
\label{sub:tomography_introduction}

Tomography is the cross-sectional imaging of an object through the use
of transmitted or reflected waves, captured by the object exposure to
the waves from a set of known angles. It has many different applications
in science, industry, and most prominently, medicine. Since the
invention of the Computed Tomography (\gls{CT}) machine in 1972, by
Hounsfield~\cite{Gunderman2006}, tomographic imaging techniques have had
a revolutionary impact, allowing doctors to see inside their patients,
without having to subject them to more invasive
procedures~\cite{Kak2001}.

Mathematical basis for tomography were set by Johannes Radon in 1917. At
the time, he postulated that  it is possible to represent a function
written in $\mathbb{R}$ in the space of straight lines, $\mathbb{L}$
through the function's line integrals. A line integral is an integral in
which the function that is being integrated is evaluated along a curved
path, a line. In the tomographic case, these line integrals represent a
measurement on a ray that traverses the Region Of Interest (\gls{ROI}).
Each set of line integrals, characterized by an incidence angle, is
called a projection (see Figure~\ref{fig:projection}). To perform a
tomographic reconstruction, the machine must take many projections
around the object. To the set of projections arranged in matrix form by
detector and projection angle, we call sinogram. All reconstruction
methods, analytical and iterative, revolve around going from reality to
sinogram to image~\cite{Bruyant2002, Kak2001, Herman1973, Herman1995,
Herman2009, Defrise2003}.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.5\textwidth]{img/png/projections.png}
    \caption{A schematic representation of a projection acquisition. In
    this image, taken from ~\cite{Herman2009}, the clear line that comes
    down at a diagonal angle is a projection.}
    \label{fig:projection}
\end{figure}

There are two broad algorithm families when it comes to tomographic
reconstruction, regarding the physics of the problem. The problem can
involve either non-diffracting sources (light travels in straight
lines), such as the X-Rays in a conventional \gls{CT} exam; or
diffracting sources, such as micro-waves or ultrasound in more
research-oriented applications~\cite{Kak2001}. In this document, I will
not address the latter family, since I will not be applying them in my
work. In the next few paragraphs, I will discuss the first family of
algorithms, and describe how an image can be reconstructed from an
object's projections when the radiation source is non-diffracting.

Let's consider the case in which we deal with a single ray of solar
light entering the atmosphere at a given point. Since the atmosphere
contains numerous absorbents and comparable atmospheric effects, the ray
changes from the point where it enters the atmosphere to the point at
which it is measured by a detector. Total absorption will depend on the
pollutant species, their cross-section and their concentration, since it
obeys Lambert-Beer's law. Looking from another angle, this absorption
is also the line integral that we will use to reconstruct our image.
With \gls{DOAS}, it is possible to measure several pollutants at the
same time, but for simplicity (and since it is one of the most studied
compounds in the field), let's consider that the single pollutant in our
atmospheric mixture is NO$_2$.

\subsection{Initial Considerations}%
\label{sub:initial_considerations}

The problem of tomographic reconstruction can be approached in a number
of ways, depending mostly on the authors. In my literary search, I have
found that Kak and Slaney~\cite{Kak2001} have certainly explained this
problem in one of the clearer ways available. Therefore, I shall base
the rest of my presentation in their writings, and complement with other
authors' notes wherever necessary.

Considering the coordinate system displayed in
Figure~\ref{fig:coordinates}. In this schematic, the object is
represented by the function $f(x, y)$. The  $(\theta, t)$ parameters can
be used to define any line in this schematic. Line AB in particular can
be written:

\begin{equation}
    \label{eq:lineAB}
    x \cdot \cos(\theta) + y \cdot \sin(\theta) = t
\end{equation}

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.7\textwidth]{img/png/coordinates.png}
    \caption{Schematic representation for coordinate setting. The image
    depicts a parallel projection setting~\cite{Kak2001a}.}
    \label{fig:coordinates}
\end{figure}

And if we were to write a line integral along this line, it would look
like Equation~\ref{eq:lineABIntegral}, the Radon transform of function
$f(x, y)$:

\begin{equation}
    \label{eq:lineABIntegral}
    P_{\theta}(t) = \int_{-\infty}^{\infty} f(x, y) \cdot \delta(x \cdot
    \cos(\theta) + y \cdot \sin(\theta) - t) dxdy
\end{equation}

Where $\delta$, the delta function, is defined in
Equation~\ref{eq:delta}.

\begin{equation}
    \label{eq:delta}
    \delta (\phi) =  
    \begin{cases}
            1, & \phi = 0\\
            0, & otherwise
    \end{cases}
\end{equation}

As I have mentioned previously, a projection is a set of line integrals
such as $P_{\theta}(t)$. Geometry plays a very important role in how the
integrals are written and solved for reconstruction. The simplest case
is the one where the set is acquired in a row, describing what is called
a parallel geometry. Another more complex case is when a single point
source is used as origin for all rays, forming a fan. This is called a
fan-beam array. There are other possible geometries, but they fall out of
the scope of this work and will therefore not be addressed any further.

\subsection{The Fourier Slice Theorem}%
\label{sub:the_fourier_slice_theorem}

The Fourier Slice Theorem (\gls{FST}) is the most important component of
the most important algorithm in tomographic inversion, the Filtered
BackProjection algorithm (\gls{FBP}). \gls{FST} is based on the equality
relation between the 
two-dimensional Fourier Transform (\gls{FT}) of the object function and
the one-dimensional \gls{FT} of the object's projection at an angle
$\theta$. Let's start by writing the 2D \gls{FT} for the object
function, Equation~\ref{eq:objectFT}, and the 1D \gls{FT} of projection
P$_\theta$, in Equation~\ref{eq:1dFTproj}.

\begin{equation}
    \label{eq:objectFT}
    F(u, v) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(x, y)
    \cdot \exp \left [ -j2\pi (ux + vy) \right ] dx dy 
\end{equation}

\begin{equation}
    \label{eq:1dFTproj}
    S_{\theta}(\omega) = \int_{-\infty}^{\infty} P_{\theta} \cdot \exp\left[
    -j2 \pi \omega t \right]
\end{equation}

For simplicity, let's consider the 2D \gls{FT} at the line defined by
$v=0$ in the frequency domain. We rewrite the 2D \gls{FT} integral as:

\begin{equation}
    \label{eq:v0}
    F(u, 0) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(x, y)
    \cdot \exp \left[  -j 2\pi  \omega ux \right] dx dy
\end{equation}

Notice that $y$ is not present in the phase factor of the \gls{FT}
expression anymore, and this means we can rearrange the integral as:

\begin{equation}
    \label{eq:v02}
    F(u, 0) = \int_{-\infty}^{\infty} \left[ \mathbf{\int_{-\infty}^{\infty}
    f(x, y) dy }\right] \cdot \exp \left[  -j 2\pi  \omega ux \right] dx 
\end{equation}

Now, the \textbf{bold} part of Equation~\ref{eq:v02} is similar to
Equation~\ref{eq:lineABIntegral}. It is precisely that equation,
considering $\theta=0$ and a constant value of $x$, as in
Equation~\ref{eq:p0}.

\begin{equation}
    \label{eq:p0}
    P_{\theta=0} (x) = \int_{-\infty}^{\infty} f(x, y) dy
\end{equation}

This in turn can be substituted in Equation~\ref{eq:v02}, finally
arriving at:

\begin{equation}
    \label{eq:FTP}
    F(u, 0) = \int_{-\infty}^{\infty} P_{\theta=0} (x) \cdot \exp \left[
    -j 2\pi ux \right] dx
\end{equation}

And this is the one-dimensional \gls{FT} for the projection at angle
$\theta=0$. Finally, the enunciation of the Fourier Slice Theorem:
\begin{center}
    \begin{minipage}{0.8\textwidth}

        \noindent\textbf{\emph{The Fourier Transform of a parallel
                projection  of an image $f(x, y)$ taken at angle
                $\theta$ gives a slice of the two-dimensional Fourier
                Transform, $F(u, v)$, subtending an angle $\theta$ with
                the $u$-axis (see Figure~\ref{fig:fst})}}

    \end{minipage}
\end{center}

\begin{figure}[htpb]
    \centering
    \includegraphics[width=.8\textwidth]{img/png/fst.png}
    \caption{The \gls{FST}, a schematic
    representation~\cite{Asl2013a}.}
    \label{fig:fst}
\end{figure}

\subsection{The Filtered BackProjection Algorithm}%
\label{sub:the_filtered_backprojection_algorithm}

\subsubsection{The rationale for \gls{FBP}}%
\label{ssub:the_rationale_for_fbp}

If one takes the \gls{FST} into account, the idea behind the \gls{FBP}
seems to appear almost naturally. Say one has a single projection and
its Fourier transform. From the \gls{FST}, this projection is the same
as the object's two-dimensional \gls{FT} in a single line. A crude
reconstruction of the original object would result if someone were to
place this projection in its right place in the Fourier domain and then
perform a two-dimensional \gls{IFT}, while assuming every other
projection to be 0. The result, in the image space, would be as if
someone had smeared the object in the projections direction.

What is really needed for a correct reconstruction is to do this many
times, with many projections. This brings a problem with the method:
smearing the object in all directions will clearly produce a wrong
\emph{accumulation} in the center of the image, since every projection
passes through the middle (remember we are still talking about parallel
geometry projections) and are summed on top of each other, but on the
outer edges, this does not occur. If one does not address this, the
image intensity levels in the reconstructed image will be severely
overestimated in the center and underestimated in the edges (due to
normalization). The solution is conceptually easy: we multiply the
Fourier transform by a weighting filter proportional to its frequency
($\omega$) and that encompasses its relevance in the global scheme of
projections. If there are $K$ projections, then it is adequate for this
value to be $\frac{2\pi\lvert\omega\rvert}{K}$. As an algorithm,
\gls{FBP} can be written as in Algorithm~\ref{alg:fbp}.

\begin{algorithm}
    \caption{The Filtered BackProjection Algorithm}
    \label{alg:fbp}
    \begin{algorithmic}
        \FORALL{$\theta, \theta \in \left\{0..180,
        \frac{180}{K}\right\}$}
        \STATE{Measure projection $P_{\theta}(t)$;}
        \STATE{FT($P_{\theta}(t)$), rendering $S_{\theta}(\omega)$}
        \STATE{Multiply by $\frac{2\pi\lvert{\omega}\rvert}{K}$;}
        \STATE{Sum the \gls{IFT} of the result in the image space.}
    \ENDFOR
    \end{algorithmic}
\end{algorithm}

\subsubsection{Fan Projections Reconstruction}%
\label{ssub:fan_projections_reconstruction}

Parallel projections, in which the object is scanned linearly from
multiple directions, have the advantage of having a relatively simple
reconstruction scheme. However, they usually result in acquisition times
which are in the order of minutes. A faster way of collecting the data
is one where all radiation emanates from a single point-source, which
rotates around the target object (as well as the detectors). There are
two types of fan-beam projections: equiangular and equally spaced. In
this project, I have only worked with equiangular processes, so I will
not include an explanation for equally spaced fan-beam projections. The
reader may find this well described (much better than I would be able
to) in ~\cite{Kak2001} and ~\cite{Herman1973}.

Consider Figure~\ref{fig:equiangular}. If our projection data were
acquired through a parallel ray geometry, we would be able to say that
ray SA belonged to a projection $P_{\theta}(t)$, in which $\theta$ and
$t$ would be written:

\begin{equation}
    \label{eq:theta_and_t}
    \theta = \beta + \gamma \quad \text{ and } \quad t = D \cdot \sin \gamma
\end{equation}

In Equation~\ref{eq:theta_and_t}, $D$ is the distance between the source
$S$ and the origin $O$; $\gamma$ is the angle of a ray within a fan and
$\beta$ is the angle that the source $S$ makes with a reference axis.
Through these relationships one can \emph{translate} the parallel
projection's FBP algorithm to the fan-beam case, which involves several
complex geometric transformations, although the overall rationale is
exactly the same.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=.8\textwidth]{img/png/fig319.png}
    \caption{Schematic representation of an equiangular fan-beam
    projection, taken from~\cite{Kak2001}.}
    \label{fig:equiangular}
\end{figure}

Another particularity of fan-beam projection data is the fact that they
can be sorted into a parallel projection. For that, one starts with the
premise that if one were to substitute the fan geometry for parallel
beams, most of the fan-beam rays would also appear in some projection of
the parallel setup. This re-sorting algorithm starts with
Equation~\ref{eq:theta_and_t}. Now, if we call a fan-beam projection
taken at angle $\beta$ $R_{\beta}(\gamma)$, and a parallel projection
taken at angle $\theta$ $P_{\theta}(t)$, one could thus write
Equation~\ref{eq:parallel_vs_fanbeam}, which can already be used to
re-sort any fan-beam projection into parallel beam geometry.

\begin{equation}
    \label{eq:parallel_vs_fanbeam}
    R_{\beta}(\gamma) = P_{\beta + \gamma}(D \cdot \sin \gamma)
\end{equation}

Let's call the angular interval between fan-beam projections can be
written $\delta\beta$, and the angular interval of rays within each fan
is written $\delta\gamma$. In the case that they are the same
($\beta=\gamma=\alpha$), then it it is the case that they can both be
replaced by multiples of that interval in
Equation~\ref{eq:parallel_vs_fanbeam}, which becomes
Equation~\ref{eq:parallel_vs_fanbeam2}.

\begin{equation}
    \label{eq:parallel_vs_fanbeam2}
    R_{m \cdot \alpha}(n \cdot\alpha) = P_{m \cdot\alpha + n
    \cdot\alpha}(D \cdot \sin n \cdot\alpha)
\end{equation}

Or, in non-mathematical notation, the n\textsuperscript{th} ray of the
m\textsuperscript{th} radial projection (R) is the same as the
n\textsuperscript{th} ray in the (m+n)\textsuperscript{th} parallel
projection. Although being much simpler than directly applying the
\gls{FBP} algorithm to the fan-beam projection data, this method has a
limitation, which is the non-uniformity of the generated parallel
projections. This can usually be corrected through
interpolation~\cite{Kak2001a}. 



\section{DOAS Tomography}%
\label{sec:doas_tomography}

During the course of this PhD, and as a credit requirement, I attended
several courses, namely in the Computer Science Department. One of them,
entitled "\emph{Advanced Software Development}" had as final requirement
the writing of a \gls{sms} on a topic of my choosing. This course
happened around the time I was starting to develop a \gls{DOAS}
tomography system, and one of the requirements of that work was to study
the state of the art to get a clear notion of how these systems were
being applied and developed globally. Therefore, this was the topic that
I chose for my \gls{sms}. The article (currently awaiting acceptance for
publication) is transcribed in full in Appendix~\ref{ap:tomDoas},  but I
include a selection of the most important parts in this section.

An \gls{sms} is a systematic first approach to a subject, and are
normally designed to retrieve a broad view of a given research area and
quantify the amount of evidence that there is in that specific field.
They share some similarities with \gls{slr} but work with much broader
data extraction and analysis procedures.Many times, they are used as
guides for primary research~\cite{Kitchenham2007}. In our case, the goal
was to understand the literary landscape of \gls{DOAS} tomographic
applications.

Figure~\ref{fig:sms_planning} denotes the whole process of conducting
an \gls{sms}. In general, \acrlong{sms} are comprised of 3 stages:
planning, conduction and reporting. The first stage is where the study
itself is parametrized. This stage does not directly represent any kind
of practical application, but it conditions and defines how the other
two stages will take place, and is probably the most important stage.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=.8\textwidth]{img/pdf/sms_planning.pdf}
    \caption{A schematic representation of an \gls{sms} writing process.
    The whole procedure is comprised of three smaller stages, which in
    turn are divided into various even smaller parts. The large arrow on the
    right represents the general evolution direction, although there is no
    rigid structure, and some back and forth is allowed and even expected.}
    \label{fig:sms_planning}
\end{figure}

There are two main components to the planning stage: the context and the
review. Broadly, these could be understood to define (respectively) how
the topics are to be searched and what it is that the authors are
looking for.

The context definition of the planning stage is when the authors define
the research questions and decide on the search protocol. The research
questions are defined according to the research goal, which in this case
was to assess the current status of the technology used in tomographic
\gls{DOAS}. A very common method for structuring the study's efforts is
the \gls{picoc} method. In our own case, this method is reflected in
Table~\ref{tab:picoc_sms}. The \gls{picoc} process rendered the original
research question, but it was not sufficiently specific in order to
conduct the search on. Therefore, we have sectioned the question into
three smaller and more manageable questions. The several questions are
detailed in Table~\ref{tab:sec_RQ}.

\begin{table}[htb]
\small
\centering
\caption{PICOC analysis.}
\label{tab:picoc_sms}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Population} & DOAS research in general.\\
\textbf{Intervention} & The papers must address tomographic DOAS.\\
\textbf{Outcome} & Status \textbf{assessment} for \textbf{DOAS tomography}.\\
\textbf{Context} & Research papers.\\\bottomrule
\end{tabular}
\end{table}

\begin{table}[htb]
\centering
\small
\caption{Research question slicing}
\label{tab:rq_slicing}
    \begin{tabularx}{\textwidth}{lX}
        \toprule
        \textbf{Original} & What is the current status of the technology used in
        tomographic DOAS? \\
        \textbf{RQ1} & Is there a typical hardware setup used in tomographic
        DOAS studies? \\
        \textbf{RQ2} & Is there a standard software used to perform these
        analysis? \\
        \textbf{RQ3} & What are the algorithms more commonly used?\\\bottomrule
    \end{tabularx}
\end{table}

The review step of the planning stage starts by the definition of the
search queries and the electronic libraries in which they would be
searched.  Preliminary searches indicated that, although the topic was
well-established as a measuring technique, literature on the subject was
not widely available. Therefore, the search query was intentionally
vague: \textbf{DOAS atmospher* tomography}, with the asterisk working as
a wild card. As to the electronic libraries, and from the same
preliminary searches, we understood that the search would have to
involve the broadest repositories available. After experimenting a
little further, we decided to base our study in the libraries presented
in Table~\ref{tab:libraries_sms}.

\begin{table}[htb]
\centering
\caption{Electronic libraries used in this study.}
\label{tab:libraries}
    \begin{tabularx}{\textwidth}{ll}
        \toprule
        \textbf{Library}          & \textbf{URL}\\
        \midrule
        Google Scholar (GS)   & https://scholar.google.com/\\
        Web of Knowledge (WoK)& https://webofknowledge.com\\
        Scopus (SD)   & https://www.scopus.com\\
        % IEEE             & http://ieeexplore.ieee.org/\\
        % AGU Publications (AGU) & http://agupubs.onlinelibrary.wiley.com/hub/\\
        \bottomrule
    \end{tabularx}
\end{table}

Exclusion and inclusion filters are what determines what papers are kept
from those that are originally found by the search process and are
therefore important parameters of an \gls{sms} study.
Table~\ref{tab:select_filters} provides a list of the filters that were
used in our study. To a certain degree, the application of these filters
are the first step of the data extraction process. Their definition is
thus part of the data extraction strategy, which determines the
guidelines with which the authors are to retrieve the target information
from the resulting literary corpus.

\begin{table}[htb]
\centering
\caption{Selection filters in use for this study's search.}
\label{tab:select_filters}
\begin{tabularx}{\textwidth}{lXl}%{@{}cll@{}}
\toprule
\multicolumn{1}{l}{} & \textbf{Criterium} & \textbf{Definition} \\ \midrule
\multirow{1}{*}{\textbf{Exc. Criteria}} & EC1 & Satellite data papers
are not accepted \\
\midrule
\multicolumn{1}{l}{\multirow{3}{*}{\textbf{Inc. Criteria}}} & IC1 &
Results must be journal papers \\
\multicolumn{1}{l}{} & IC2 & Results must be about Tomographic DOAS \\ 
\multicolumn{1}{l}{} & IC3 & Results must be written in English \\
\bottomrule
\end{tabularx}
\end{table}

One of the most important parts of the data extraction strategy is how
to evaluate each paper with respect to its quality. For this study, and
although this is anything but an objectively defined topic, evaluation
was performed according to the formula in
Equation~\ref{eq:qual_formula}, where $Q_i$ is the paper's quartile and
$C_i$ is the number of citations that each particular study has gathered
throughout the years, which are represented by $Age_i$. In this
equation, $S$ represents the final quality score.

\begin{equation}
    \label{eq:qual_formula}
    S = Q_{i} \cdot \frac{C_i}{Age_i}
\end{equation}

\todo{Complete this}





