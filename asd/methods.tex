%!TEX root=../template.tex

\section{Methods}\label{sec:methods}

In the elaboration of this article, we took the three normal stages of
SLR conduction: planning, conduction and reporting. The first stage
involves making the decisions that guide the rest of the process; the
conduction phase is comprised of the actual gathering of data, using the
protocol defined in the first stage. The final section of the study is
basically the writing and the publishing of the results. In this
section, we present the methods used in the study and their rationale,
which roughly corresponds to the planning stage.

\subsection{Objectives}
\label{sub:objectives}

An MS always aims to answer its research questions in a broad but
definite way. It is a way of understanding a given field of research,
and being able to systematise how this understanding is achieved.

As stated before, this is an MS aimed at characterising DOAS tomography
general status. In doing this, we pretend to get a clearer image of what
has been done and what should be attempted next, hopefully managing a
sort of roadmap for future research contributions.
%End Objectives

\subsection{Research Questions}
\label{sub:research_questions_and_search_queries}

We have begun by defining the goals for our study, and structuring them
with a PICOC (Population, Intervention, Context, Outcome and Comparison)
analysis, which is summarised in Table~\ref{tab:picoc}. This analysis
led us to our research goal: \textit{to assess the technological status
    of the DOAS tomography technique}.
    
We used this goal statement as a primer to our research question,
which was then formulated as: \textbf{what is the current status of
the technology used in tomographic DOAS?}

\begin{table}[htb]
\small
\centering
\caption{PICOC analysis.}
\label{tab:picoc}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Population} & DOAS research in general.\\ 
\textbf{Intervention} & The papers must address tomographic DOAS.\\
\textbf{Outcome} & Status \textbf{assessment} for \textbf{DOAS tomography}.\\
\textbf{Context} & Research papers.\\\bottomrule
\end{tabular}
\end{table}

Now, this question is too vague to pursue in a systematic fashion, so we
had to slice it into smaller and more objective chunks. This sectioning
is presented in Table~\ref{tab:rq_slicing}.

\begin{table}[htb]
\centering
\small
\caption{Research question slicing}
\label{tab:rq_slicing}
    \begin{tabularx}{\textwidth}{lX}
        \toprule
        \textbf{Original} & What is the current status of the technology used in
        tomographic DOAS? \\
        \textbf{RQ1} & Is there a typical hardware setup used in tomographic
        DOAS studies? \\
        \textbf{RQ2} & Is there a standard software used to perform these
        analysis? \\
        \textbf{RQ3} & What are the algorithms more commonly used?\\\bottomrule
    \end{tabularx}
\end{table}

The research question is one of the most important steps in planning a
Systematic Literature Review, but it cannot be entered into a library's
search box. Therefore, we have to define our search terms before we can
make any effort of answering our questions.

\subsection{Search Query Definition, Library Selection and Filter Definition}
\label{sub:library_selection_and_filter_definition}

In the case of this study, the search terms were selected in order to
purposefuly maintain a broad scope, so that we could retrieve a high
number of relevant studies. The selected search terms were: \textbf{DOAS
atmospher* tomography}\footnote{The asterisk acts as a wildcard.}. The
search query was entered into 5 academic search engines, as shown in
Table~\ref{tab:libraries}.

\begin{table}[htb]
\centering
\caption{Electronic libraries used in this study.}
\label{tab:libraries}
\begin{tabularx}{\textwidth}{ll}
\toprule
\textbf{Library}          & \textbf{URL}\\
\midrule
Google Scholar (GS)   & https://scholar.google.com/\\
Web of Knowledge (WoK)& https://webofknowledge.com\\
Science Direct (SD)   & https://www.sciencedirect.com\\
IEEE             & http://ieeexplore.ieee.org/\\
AGU Publications (AGU) & http://agupubs.onlinelibrary.wiley.com/hub/\\
\bottomrule
\end{tabularx}
\end{table}

After setting Table~\ref{tab:libraries} libraries, it was time to define
our article selection criteria, which are summarised in
Table~\ref{tab:select_filters}. We began with 2 Inclusion Criteria (IC)
and 3 Exclusion Criteria. The IC determined that our selected papers
would have to be journal articles (thus exluding thesis, white papers,
patents and other documents) and that these articles should be on the
topic of Tomographic DOAS. The EC dictated that no selected paper should
include volcanology studies or satellite data analysis (these have
particularities which we do not want to approach in this study) and that
no other language than English will be accepted.

During the course of the search, however, we had to include another two
EC. The first was included in the Google Scholar search, where we
understood that papers from a certain publisher were not accessible. The
second came in the subsequent searches, when it became clear that most
papers had already been retrieved by the GS search. 

\begin{table}[htb]
\centering
\caption{Selection filters in use for this study's search.}
\label{tab:select_filters}
\begin{tabularx}{\textwidth}{lXl}%{@{}cll@{}}
\toprule
\multicolumn{1}{l}{} & \textbf{Criterium} & \textbf{Definition} \\ \midrule
\multirow{5}{*}{\textbf{Exc. Criteria}} & EC1 & Duplicate in Scholar \\
 & EC2 & Non English articles are not accepted \\
 & EC3 & Volcanology papers are not accepted \\
 & EC4 & Satellite data papers are not accepted \\
 & EC5 & CNKI published articles are not accepted \\ \midrule
\multicolumn{1}{l}{\multirow{2}{*}{\textbf{Inc. Criteria}}} & IC1 & Results must be articles \\
\multicolumn{1}{l}{} & IC2 & Results must be about Tomographic DOAS \\ \bottomrule
\end{tabularx}
\end{table}

\subsection{Data Extraction Strategy}
\label{sub:data_extraction_strategy}

The data extraction process is a key part of any systematic review,
whether an SLR or an MS. It determines how each article is approached
with regard to its content, before any information is retrieved. In our
case, our strategy took place in two separate moments: an initial
screening, in which we would assess contents as expressed by the
articles' abstract; and a second moment, in which we performed a full
article read. Special attention was given to explicit sections covering
our target topics (equipment, algorithm and software). 

% End of data extraction strategy

\subsection{Quality Assessment}
\label{sub:quality_assessment}

It is very difficult to assess a paper's quality, and to rank it
accordingly. However, for this review in particular, we have decided to
follow Souza's approach~\cite{Souza2019} and adopt a similar evaluation
method.  Table~\ref{tab:quality_assessment_criteria} contains the used
criteria. 

In our evaluation model, we took into account both general ans specific
criteria. The former addresses an article's contributions to our
particular SLR; the latter targets the actual content of that article. 

In order to measure the contribution of each individual paper to our
study (our general criterium), we have assessed its number of citations
in all the other selected papers. This is a valid measurement of a
papers impact in the study, but it might become difficult to implement
if a high number of articles are selected for the final stage.
Contentwise (specific criteria), we have defined our scoring model
according to the Research Question separation explained in
Subsection~\ref{sub:research_questions_and_search_queries}.

\begin{table}
    \centering
    \caption{Quality assessment criteria presentation.}
    \label{tab:quality_assessment_criteria}
        \begin{small}
            \begin{tabularx}{\textwidth}{lXXr}%{@{}llll@{}}
                \textbf{Criterium Type} & \textbf{Criterium (Weight)} & \textbf{Decision Factor} & \textbf{Score} \\ \midrule
                \multirow{4}{*}{\textbf{General Criteria}} &
                \multirow{4}{*}{Contribution to this SLR (0,2)} & cited in study: more than three times & 1 \\
                 &  & cited in study: three times & 0,75 \\
                 &  & cited in study: twice & 0,5 \\
                 &  & cited in study: once or less & 0,25 \\ \midrule
                \multicolumn{1}{l}{\multirow{10}{*}{\textbf{Specific Criteria}}}
                & \multirow{4}{*}{Algorithm description (0,6)} & Detailed & 1 \\
                \multicolumn{1}{l}{} &  & Semi-Detailed & 0,4 \\
                \multicolumn{1}{l}{} &  & Mentioned & 0,2 \\
                \multicolumn{1}{l}{} &  & None & 0 \\ \cmidrule(l){2-4} 
                \multicolumn{1}{l}{} &
                \multicolumn{1}{c}{\multirow{4}{*}{Instrument description (0,2)}} & Detailed & 1 \\
                \multicolumn{1}{l}{} & \multicolumn{1}{c}{} & Semi-Detailed & 0,4 \\
                \multicolumn{1}{l}{} & \multicolumn{1}{c}{} & Mentioned & 0,2 \\
                \multicolumn{1}{l}{} & \multicolumn{1}{c}{} & None & 0 \\ \cmidrule(l){2-4} 
                \multicolumn{1}{l}{} & \multirow{2}{*}{Software Description (0,1)} & Mentioned & 1 \\
                \multicolumn{1}{l}{} &  & None & 0 \\ \bottomrule
            \end{tabularx}    
        \end{small}
    
\end{table}

In our study's case, distinction between Specific and General was not
sufficient to adequately separate scores according to importance. It was
necessary to introduce scoring weights for that end. These weights are
also shown in Table~\ref{tab:quality_assessment_criteria} and were set
according to the goals of our SLR, meaning that the tomographic element
is the most important.

In the end, a paper's total score comes from the formula described by
Equation~\ref{eq:score}.

\begin{equation}
    \centering
    \label{eq:score}
    Total Score = \sum_{i} w_{i} \cdot S_{i}
\end{equation}

Where $S_i$ and $w_i$ are a paper's score and weight for a given
criterium, respectively.

Finally, we shall discuss the different weights given to each criterium
and the different ways in which they are evaluated. The most important
aspect that we are trying to assess is the algorithm, which defines the
whole tomographic process and the results achievable by the studies.

A detailed algorithmic description includes the mathematical basis as
well as a complete description of required adaptations, both on the
mathematical level and on a conceptual method.

Instrument description is also an important criterium, since it is with
it that scientists retrieve the information they will afterwards process
tomographically, through the algorithm.

It is sometimes difficult to establish how good an instrument descrition
is. A too detailed description can be just as bad as a non-sufficient
one, if the equipment options are not correctly presented.

That being said, we have considered a detailed description one that
includes explicit mention to the composition of the optical system and
its assembly details, together with the analysing hardware (e.g.
spectrometers) configuration and capabilities.

The least important of the technical features under evaluation is the
software. This is because theoretically, results would be the same
independently of the software in use. We have included this feature in
the study as a way of identifying if there was some kind of software
prevalence in the community. In this study, software is binarily
assessed: either the scrutinised study mentions it or not.

Finally, we evaluate the contribution of each article to this study.
Since DOAS tomography is a field with a relatively low number of
players, it can be expected that there are many cross citations. We have
introduced this as a method of measuring an article's relative
importance within this mapping study, simply by counting the number of
times a cross citation occurs.
